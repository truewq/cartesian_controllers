{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f66f7a20",
      "metadata": {
        "origin_pos": 0,
        "id": "f66f7a20"
      },
      "source": [
        "# 自定义层\n",
        "\n",
        "深度学习成功背后的一个因素是神经网络的灵活性：\n",
        "我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。\n",
        "例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。\n",
        "有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。\n",
        "在这些情况下，必须构建自定义层。本节将展示如何构建自定义层。\n",
        "\n",
        "## 不带参数的层\n",
        "\n",
        "首先，我们(**构造一个没有任何参数的自定义层**)。\n",
        "回忆一下在 :numref:`sec_model_construction`对块的介绍，\n",
        "这应该看起来很眼熟。\n",
        "下面的`CenteredLayer`类要从其输入中减去均值。\n",
        "要构建它，我们只需继承基础层类并实现前向传播功能。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc3b353a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:16.604374Z",
          "iopub.status.busy": "2023-08-18T07:07:16.603752Z",
          "iopub.status.idle": "2023-08-18T07:07:17.492480Z",
          "shell.execute_reply": "2023-08-18T07:07:17.491482Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "cc3b353a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class CenteredLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X - X.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c321cf",
      "metadata": {
        "origin_pos": 5,
        "id": "a3c321cf"
      },
      "source": [
        "让我们向该层提供一些数据，验证它是否能按预期工作。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dec68045",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:17.497408Z",
          "iopub.status.busy": "2023-08-18T07:07:17.497077Z",
          "iopub.status.idle": "2023-08-18T07:07:17.508357Z",
          "shell.execute_reply": "2023-08-18T07:07:17.507175Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "dec68045",
        "outputId": "1d50d47d-1b61-4abf-fecd-27876a209f19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-3., -2., -1.,  0.,  1.,  2.,  3.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "layer = CenteredLayer()\n",
        "layer(torch.FloatTensor([1, 2, 3, 4, 5]))\n",
        "layer(torch.FloatTensor([1, 2, 3, 4, 5,6,7]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d38600d",
      "metadata": {
        "origin_pos": 10,
        "id": "9d38600d"
      },
      "source": [
        "现在，我们可以[**将层作为组件合并到更复杂的模型中**]。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1b903c3c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:17.513247Z",
          "iopub.status.busy": "2023-08-18T07:07:17.512547Z",
          "iopub.status.idle": "2023-08-18T07:07:17.518968Z",
          "shell.execute_reply": "2023-08-18T07:07:17.517886Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "1b903c3c"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c48076d",
      "metadata": {
        "origin_pos": 14,
        "id": "4c48076d"
      },
      "source": [
        "作为额外的健全性检查，我们可以在向该网络发送随机数据后，检查均值是否为0。\n",
        "由于我们处理的是浮点数，因为存储精度的原因，我们仍然可能会看到一个非常小的非零数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6ab302a0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:17.523517Z",
          "iopub.status.busy": "2023-08-18T07:07:17.523140Z",
          "iopub.status.idle": "2023-08-18T07:07:17.534718Z",
          "shell.execute_reply": "2023-08-18T07:07:17.533593Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "6ab302a0",
        "outputId": "32d6c0cf-8ee3-4e2c-d769-5712894fb59e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.7253e-09, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "Y = net(torch.rand(4, 8))\n",
        "Y.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca107571",
      "metadata": {
        "origin_pos": 19,
        "id": "ca107571"
      },
      "source": [
        "## [**带参数的层**]\n",
        "\n",
        "以上我们知道了如何定义简单的层，下面我们继续定义具有参数的层，\n",
        "这些参数可以通过训练进行调整。\n",
        "我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。\n",
        "比如管理访问、初始化、共享、保存和加载模型参数。\n",
        "这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。\n",
        "\n",
        "现在，让我们实现自定义版本的全连接层。\n",
        "回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。\n",
        "在此实现中，我们使用修正线性单元作为激活函数。\n",
        "该层需要输入参数：`in_units`和`units`，分别表示输入数和输出数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8c4a7999",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:17.539101Z",
          "iopub.status.busy": "2023-08-18T07:07:17.538729Z",
          "iopub.status.idle": "2023-08-18T07:07:17.546162Z",
          "shell.execute_reply": "2023-08-18T07:07:17.545105Z"
        },
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ],
        "id": "8c4a7999"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):\n",
        "    def __init__(self, in_units, units):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
        "        self.bias = nn.Parameter(torch.randn(units,))\n",
        "    def forward(self, X):\n",
        "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
        "        return F.relu(linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "442183c6",
      "metadata": {
        "origin_pos": 25,
        "tab": [
          "pytorch"
        ],
        "id": "442183c6"
      },
      "source": [
        "接下来，我们实例化`MyLinear`类并访问其模型参数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4490005a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:17.550522Z",
          "iopub.status.busy": "2023-08-18T07:07:17.550152Z",
          "iopub.status.idle": "2023-08-18T07:07:17.558364Z",
          "shell.execute_reply": "2023-08-18T07:07:17.557338Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "4490005a",
        "outputId": "6df9ff02-1934-4ced-a711-5baae04e91d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0470,  3.4404,  0.0284],\n",
            "        [-0.5516, -0.3267, -0.0415],\n",
            "        [ 0.6444,  0.7150,  0.5598],\n",
            "        [ 0.4184, -0.1816, -0.8392],\n",
            "        [ 1.5778,  2.9411, -0.3489]], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1997, 4.3156, 0.6216],\n",
              "        [1.2237, 4.5207, 0.6360]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "linear = MyLinear(5, 3)\n",
        "print(linear.weight)\n",
        "\n",
        "linear(torch.rand(2, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dcc8fd9",
      "metadata": {
        "origin_pos": 30,
        "id": "7dcc8fd9"
      },
      "source": [
        "我们可以[**使用自定义层直接执行前向传播计算**]。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "25f2aabf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:17.562706Z",
          "iopub.status.busy": "2023-08-18T07:07:17.562337Z",
          "iopub.status.idle": "2023-08-18T07:07:17.570015Z",
          "shell.execute_reply": "2023-08-18T07:07:17.568916Z"
        },
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "25f2aabf",
        "outputId": "9ee7c1f8-4b34-4e88-a891-20ae503be4f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8923, 5.0641, 0.8649],\n",
              "        [0.0707, 3.8562, 1.1139]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "linear(torch.rand(2, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c92ac1e0",
      "metadata": {
        "origin_pos": 35,
        "id": "c92ac1e0"
      },
      "source": [
        "我们还可以(**使用自定义层构建模型**)，就像使用内置的全连接层一样使用自定义层。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "fb2953e8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:17.574378Z",
          "iopub.status.busy": "2023-08-18T07:07:17.574000Z",
          "iopub.status.idle": "2023-08-18T07:07:17.582792Z",
          "shell.execute_reply": "2023-08-18T07:07:17.581735Z"
        },
        "origin_pos": 37,
        "tab": [
          "pytorch"
        ],
        "id": "fb2953e8",
        "outputId": "04494b13-50c8-4eca-822e-55a700858d52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
        "net(torch.rand(2, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a23d1ab",
      "metadata": {
        "origin_pos": 40,
        "id": "5a23d1ab"
      },
      "source": [
        "## 小结\n",
        "\n",
        "* 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层，其行为与深度学习框架中的任何现有层不同。\n",
        "* 在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。\n",
        "* 层可以有局部参数，这些参数可以通过内置函数创建。\n",
        "\n",
        "## 练习\n",
        "\n",
        "1. 设计一个接受输入并计算张量降维的层，它返回$y_k = \\sum_{i, j} W_{ijk} x_i x_j$。\n",
        "1. 设计一个返回输入数据的傅立叶系数前半部分的层。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#练习一：设计一个接受输入并计算张量降维的层，它返回 yk=∑i,jWijkxixj 。\n",
        "\n",
        "class Exercise1(nn.Module):\n",
        "    def __init__(self, input, output):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(input, input, output))\n",
        "\n",
        "    def forward(self, X):\n",
        "        print(X)\n",
        "        print(self.weight)\n",
        "        batch_size = X[0]\n",
        "        X_i = X.unsqueeze(-1)# (batch_size, input_dim, 1)\n",
        "        X_j = X.unsqueeze(-2)# (batch_size, 1, input_dim)\n",
        "        print(f'xixj={(X_i*X_j).shape}')\n",
        "        print(f'xi:{X_i},\\nxj:{X_j}')\n",
        "        print(f'weight:{self.weight.shape}')\n",
        "        quadratic_terms = torch.einsum('bij,ijk->bk', X_i * X_j, self.weight)\n",
        "\n",
        "        return quadratic_terms\n",
        "\n",
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class QuadraticLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        # 初始化权重参数 W，形状为 (input_dim, input_dim, output_dim)\n",
        "        self.weight = nn.Parameter(torch.randn(input_dim, input_dim, output_dim))\n",
        "        # 初始化偏置参数\n",
        "        self.bias = nn.Parameter(torch.randn(output_dim,))\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        batch_size = X.shape[0]  # 获取批量大小\n",
        "        # 扩展 X 以便进行广播计算\n",
        "        X_i = X.unsqueeze(-1)  # (batch_size, input_dim, 1)\n",
        "        X_j = X.unsqueeze(-2)  # (batch_size, 1, input_dim)\n",
        "        # 计算所有组合 x_i * x_j 并与权重 W 相乘\n",
        "        quadratic_terms = torch.einsum('bij,ijk->bk', X_i * X_j, self.weight)\n",
        "        # 加上偏置并返回\n",
        "        return quadratic_terms + self.bias\n",
        "'''\n",
        "# 测试\n",
        "input_dim, output_dim = 4, 3\n",
        "X = torch.rand(2, input_dim)  # 输入维度为 4，批量大小为 2\n",
        "layer = Exercise1(input_dim, output_dim)\n",
        "output = layer(X)\n",
        "print(f\"outputshape:{output.shape}\")  # Use an f-string to print the shape\n",
        "print(f\"output:\\n{output}\") # Use an f-string to print the output tensor\n"
      ],
      "metadata": {
        "id": "kI1867L4oiqH",
        "outputId": "4ecb624a-4de7-4a3e-c337-e9490cab0f21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kI1867L4oiqH",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8496, 0.0747, 0.7134, 0.9193],\n",
            "        [0.0827, 0.3759, 0.6460, 0.0294]])\n",
            "Parameter containing:\n",
            "tensor([[[-1.3716, -1.2201,  0.5644],\n",
            "         [ 0.4686,  0.0359,  0.5512],\n",
            "         [-1.7843,  0.3642, -1.0721],\n",
            "         [ 1.5278,  0.7624, -0.3082]],\n",
            "\n",
            "        [[ 1.6962, -0.1333,  0.1517],\n",
            "         [-0.0707, -0.1057,  0.2990],\n",
            "         [ 2.9393, -1.6836, -0.4789],\n",
            "         [-2.4966,  0.7081, -1.3390]],\n",
            "\n",
            "        [[-0.0251, -1.1284,  1.9158],\n",
            "         [-0.1191,  3.0230,  0.1689],\n",
            "         [ 1.0323, -1.0270,  0.1542],\n",
            "         [ 0.2380,  0.4777,  0.0718]],\n",
            "\n",
            "        [[-0.5592,  2.7877, -0.0919],\n",
            "         [-0.2329,  1.2463, -0.1531],\n",
            "         [ 1.1700, -0.9133, -0.4053],\n",
            "         [ 0.8931, -0.0502, -0.0064]]], requires_grad=True)\n",
            "xixj=torch.Size([2, 4, 4])\n",
            "xi:tensor([[[0.8496],\n",
            "         [0.0747],\n",
            "         [0.7134],\n",
            "         [0.9193]],\n",
            "\n",
            "        [[0.0827],\n",
            "         [0.3759],\n",
            "         [0.6460],\n",
            "         [0.0294]]]),\n",
            "xj:tensor([[[0.8496, 0.0747, 0.7134, 0.9193]],\n",
            "\n",
            "        [[0.0827, 0.3759, 0.6460, 0.0294]]])\n",
            "weight:torch.Size([4, 4, 3])\n",
            "outputshape:torch.Size([2, 3])\n",
            "output:\n",
            "tensor([[ 0.9733,  0.7768,  0.3879],\n",
            "        [ 1.0665, -0.1485,  0.0783]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5d22c2",
      "metadata": {
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ],
        "id": "2d5d22c2"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1835)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}