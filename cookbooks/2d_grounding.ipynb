{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6ad3b52",
      "metadata": {
        "id": "e6ad3b52"
      },
      "source": [
        "### Spatial Understanding with Qwen3-VL\n",
        "\n",
        "This notebook showcases Qwen3-VL's advanced spatial localization abilities, including accurate object detection, specific target grounding within images.\n",
        "\n",
        "First of all, we list the major updates of Qwen3-VL's spatial understanding abilities as follows:\n",
        "* Coordinate System: Qwen3-VL's default coordinate system has been changed from the absolute coordinates used in Qwen2.5-VL to relative coordinates ranging from 0 to 1000. (You don't need to calculate the resized_w)\n",
        "* Multi-Target Grounding: Qwen3-VL has improved its multi-target grounding ability.\n",
        "\n",
        "Now, Let's see how it integrates visual and linguistic understanding to interpret complex scenes effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14ee4e7-3706-45c2-9fd1-cc49b0f00fd0",
      "metadata": {
        "id": "e14ee4e7-3706-45c2-9fd1-cc49b0f00fd0"
      },
      "source": [
        "#### \\[Setup\\]\n",
        "* step1: Prepare the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2d7a1ed6-b782-4516-874f-8864fa13da56",
      "metadata": {
        "id": "2d7a1ed6-b782-4516-874f-8864fa13da56"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/huggingface/transformers\n",
        "# !pip install qwen-vl-utils\n",
        "# !pip install openai\n",
        "# pip install qwen-vl-utils\n",
        "# pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5c878bf",
      "metadata": {
        "id": "a5c878bf"
      },
      "source": [
        "* step2: Load visualization utils."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "07044e07",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T12:15:56.383829Z",
          "iopub.status.busy": "2025-01-29T12:15:56.383261Z",
          "iopub.status.idle": "2025-01-29T12:15:58.004390Z",
          "shell.execute_reply": "2025-01-29T12:15:58.003489Z",
          "shell.execute_reply.started": "2025-01-29T12:15:56.383805Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07044e07",
        "outputId": "275d276d-61c0-465a-ba37-3600a6516a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-noto-cjk is already the newest version (1:20220127+repack1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# @title Plotting Util\n",
        "\n",
        "# Get Noto JP font to display janapese characters\n",
        "!apt-get install fonts-noto-cjk  # For Noto Sans CJK JP\n",
        "\n",
        "#!apt-get install fonts-source-han-sans-jp # For Source Han Sans (Japanese)\n",
        "\n",
        "import json\n",
        "import random\n",
        "import io\n",
        "import ast\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from PIL import ImageColor\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
        "\n",
        "def decode_json_points(text: str):\n",
        "    \"\"\"Parse coordinate points from text format\"\"\"\n",
        "    try:\n",
        "        # 清理markdown标记\n",
        "        if \"```json\" in text:\n",
        "            text = text.split(\"```json\")[1].split(\"```\")[0]\n",
        "\n",
        "        # 解析JSON\n",
        "        data = json.loads(text)\n",
        "        points = []\n",
        "        labels = []\n",
        "\n",
        "        for item in data:\n",
        "            if \"point_2d\" in item:\n",
        "                x, y = item[\"point_2d\"]\n",
        "                points.append([x, y])\n",
        "\n",
        "                # 获取label，如果没有则使用默认值\n",
        "                label = item.get(\"label\", f\"point_{len(points)}\")\n",
        "                labels.append(label)\n",
        "\n",
        "        return points, labels\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return [], []\n",
        "\n",
        "\n",
        "def plot_bounding_boxes(im, bounding_boxes):\n",
        "    \"\"\"\n",
        "    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n",
        "\n",
        "    Args:\n",
        "        img_path: The path to the image file.\n",
        "        bounding_boxes: A list of bounding boxes containing the name of the object\n",
        "         and their positions in normalized [y1 x1 y2 x2] format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the image\n",
        "    img = im\n",
        "    width, height = img.size\n",
        "    print(img.size)\n",
        "    # Create a drawing object\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    # Define a list of colors\n",
        "    colors = [\n",
        "    'red',\n",
        "    'green',\n",
        "    'blue',\n",
        "    'yellow',\n",
        "    'orange',\n",
        "    'pink',\n",
        "    'purple',\n",
        "    'brown',\n",
        "    'gray',\n",
        "    'beige',\n",
        "    'turquoise',\n",
        "    'cyan',\n",
        "    'magenta',\n",
        "    'lime',\n",
        "    'navy',\n",
        "    'maroon',\n",
        "    'teal',\n",
        "    'olive',\n",
        "    'coral',\n",
        "    'lavender',\n",
        "    'violet',\n",
        "    'gold',\n",
        "    'silver',\n",
        "    ] + additional_colors\n",
        "\n",
        "    # Parsing out the markdown fencing\n",
        "    bounding_boxes = parse_json(bounding_boxes)\n",
        "\n",
        "    font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
        "\n",
        "    try:\n",
        "      json_output = ast.literal_eval(bounding_boxes)\n",
        "    except Exception as e:\n",
        "      end_idx = bounding_boxes.rfind('\"}') + len('\"}')\n",
        "      truncated_text = bounding_boxes[:end_idx] + \"]\"\n",
        "      json_output = ast.literal_eval(truncated_text)\n",
        "\n",
        "    if not isinstance(json_output, list):\n",
        "      json_output = [json_output]\n",
        "\n",
        "    # Iterate over the bounding boxes\n",
        "    for i, bounding_box in enumerate(json_output):\n",
        "      # Select a color from the list\n",
        "      color = colors[i % len(colors)]\n",
        "\n",
        "      # Convert normalized coordinates to absolute coordinates\n",
        "      abs_y1 = int(bounding_box[\"bbox_2d\"][1] / 1000 * height)\n",
        "      abs_x1 = int(bounding_box[\"bbox_2d\"][0] / 1000 * width)\n",
        "      abs_y2 = int(bounding_box[\"bbox_2d\"][3] / 1000 * height)\n",
        "      abs_x2 = int(bounding_box[\"bbox_2d\"][2] / 1000 * width)\n",
        "\n",
        "      if abs_x1 > abs_x2:\n",
        "        abs_x1, abs_x2 = abs_x2, abs_x1\n",
        "\n",
        "      if abs_y1 > abs_y2:\n",
        "        abs_y1, abs_y2 = abs_y2, abs_y1\n",
        "\n",
        "      # Draw the bounding box\n",
        "      draw.rectangle(\n",
        "          ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=3\n",
        "      )\n",
        "\n",
        "      # Draw the text\n",
        "      if \"label\" in bounding_box:\n",
        "        draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)\n",
        "\n",
        "    # Display the image\n",
        "    img.show()\n",
        "\n",
        "\n",
        "def plot_points(im, text):\n",
        "  img = im\n",
        "  width, height = img.size\n",
        "  draw = ImageDraw.Draw(img)\n",
        "  colors = [\n",
        "    'red', 'green', 'blue', 'yellow', 'orange', 'pink', 'purple', 'brown', 'gray',\n",
        "    'beige', 'turquoise', 'cyan', 'magenta', 'lime', 'navy', 'maroon', 'teal',\n",
        "    'olive', 'coral', 'lavender', 'violet', 'gold', 'silver',\n",
        "  ] + additional_colors\n",
        "\n",
        "  points, descriptions = decode_json_points(text)\n",
        "  print(\"Parsed points: \", points)\n",
        "  print(\"Parsed descriptions: \", descriptions)\n",
        "  if points is None or len(points) == 0:\n",
        "    img.show()\n",
        "    return\n",
        "\n",
        "  font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
        "\n",
        "  for i, point in enumerate(points):\n",
        "    color = colors[i % len(colors)]\n",
        "    abs_x1 = int(point[0])/1000 * width\n",
        "    abs_y1 = int(point[1])/1000 * height\n",
        "    radius = 2\n",
        "    draw.ellipse([(abs_x1 - radius, abs_y1 - radius), (abs_x1 + radius, abs_y1 + radius)], fill=color)\n",
        "    draw.text((abs_x1 - 20, abs_y1 + 6), descriptions[i], fill=color, font=font)\n",
        "\n",
        "  img.show()\n",
        "\n",
        "def plot_points_json(im, text):\n",
        "  img = im\n",
        "  width, height = img.size\n",
        "  draw = ImageDraw.Draw(img)\n",
        "  colors = [\n",
        "    'red', 'green', 'blue', 'yellow', 'orange', 'pink', 'purple', 'brown', 'gray',\n",
        "    'beige', 'turquoise', 'cyan', 'magenta', 'lime', 'navy', 'maroon', 'teal',\n",
        "    'olive', 'coral', 'lavender', 'violet', 'gold', 'silver',\n",
        "  ] + additional_colors\n",
        "  font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
        "\n",
        "  text = text.replace('```json', '')\n",
        "  text = text.replace('```', '')\n",
        "  data = json.loads(text)\n",
        "  for item in data:\n",
        "    point_2d = item['point_2d']\n",
        "    label = item['label']\n",
        "    x, y = int(point_2d[0] / 1000 * width), int(point_2d[1] / 1000 * height)\n",
        "    radius = 2\n",
        "    draw.ellipse([(x - radius, y - radius), (x + radius, y + radius)], fill=colors[0])\n",
        "    draw.text((x + 2*radius, y + 2*radius), label, fill=colors[0], font=font)\n",
        "\n",
        "  img.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Parsing JSON output\n",
        "def parse_json(json_output):\n",
        "    # Parsing out the markdown fencing\n",
        "    lines = json_output.splitlines()\n",
        "    for i, line in enumerate(lines):\n",
        "        if line == \"```json\":\n",
        "            json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n",
        "            json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n",
        "            break  # Exit the loop once \"```json\" is found\n",
        "    return json_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f554b4",
      "metadata": {
        "id": "b6f554b4"
      },
      "source": [
        "* step3: Prepare utils for Dashscope API call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e829b782-0be7-4bc6-a576-6b815323376e",
      "metadata": {
        "ExecutionIndicator": {
          "show": false
        },
        "execution": {
          "iopub.execute_input": "2025-01-29T11:51:26.514720Z",
          "iopub.status.busy": "2025-01-29T11:51:26.514049Z",
          "iopub.status.idle": "2025-01-29T11:51:55.411363Z",
          "shell.execute_reply": "2025-01-29T11:51:55.410649Z",
          "shell.execute_reply.started": "2025-01-29T11:51:26.514696Z"
        },
        "tags": [],
        "id": "e829b782-0be7-4bc6-a576-6b815323376e"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "#import oss2\n",
        "import os\n",
        "import copy\n",
        "import traceback\n",
        "import time\n",
        "from openai import OpenAI\n",
        "#from oss2.credentials import EnvironmentVariableCredentialsProvider\n",
        "\n",
        "## Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
        "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-c40e805763a44dd38b5ca94c55d1588a\"\n",
        "\n",
        "DASHSCOPE_API_KEY = os.environ.get('DASHSCOPE_API_KEY', 'empty')\n",
        "\n",
        "CALL_URL = 'https://dashscope-intl.aliyuncs.com/compatible-mode/v1'\n",
        "HEADERS = {\n",
        "    'Content-Type': 'application/json',\n",
        "    \"Authorization\": f\"Bearer {DASHSCOPE_API_KEY}\"\n",
        "}\n",
        "\n",
        "def dash_call(**kwargs):\n",
        "    payload = copy.deepcopy(kwargs)\n",
        "    assert 'model' in payload\n",
        "    max_try = 10\n",
        "    for i in range(max_try):\n",
        "        try:\n",
        "            ret = requests.post(CALL_URL, json=payload,\n",
        "                                headers=HEADERS, timeout=180)\n",
        "            if ret.status_code != 200:\n",
        "                raise Exception(f\"http status_code: {ret.status_code}\\n{ret.content}\")\n",
        "            ret_json = ret.json()\n",
        "            gen_content = ret_json.get(\"output\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "            result = ''.join(output.get('text', '') for output in gen_content if 'text' in output)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(traceback.format_exc())\n",
        "            time.sleep(10)\n",
        "    raise Exception('Max Retry!!!')\n",
        "\n",
        "\n",
        "def inference_with_dashscope_api(img_url, prompt, min_pixels=64 * 32 * 32, max_pixels=9800* 32 * 32):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    # You can set the min_pixels and max_pixels to control the size of the image according to your use case.\n",
        "                    \"image\": img_url, \"min_pixels\": min_pixels, \"max_pixels\": max_pixels\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    kwargs = dict(\n",
        "        model='qwen3-vl-235b-a22b-instruct',\n",
        "        input={\"messages\": messages}\n",
        "    )\n",
        "    try:\n",
        "        model_response = dash_call(**kwargs)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n",
        "    return model_response\n",
        "\n",
        "\n",
        "def inference_with_openai_api(img_url, prompt, min_pixels=64 * 32 * 32, max_pixels=9800* 32 * 32):\n",
        "    import base64\n",
        "    import os\n",
        "    if os.path.exists(img_url):\n",
        "        with open(img_url, \"rb\") as image_file:\n",
        "            base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "    elif img_url.startswith(\"http://\") or img_url.startswith(\"https://\"):\n",
        "        response = requests.get(img_url)\n",
        "        response.raise_for_status()\n",
        "        base64_image = base64.b64encode(response.content).decode(\"utf-8\")\n",
        "    else:\n",
        "        raise ValueError(\"Invalid image URL\")\n",
        "    client = OpenAI(\n",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                    },\n",
        "                    \"min_pixels\": min_pixels,\n",
        "                    \"max_pixels\": max_pixels\n",
        "                },\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"qwen3-vl-235b-a22b-instruct\",  # 可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/models\n",
        "        messages=messages,\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e78c67",
      "metadata": {
        "id": "20e78c67"
      },
      "source": [
        "#### 1. Multi-Target Object Detection\n",
        "Let's start with the newly-updated multi-target grounding ability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "225f4d29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "225f4d29",
        "outputId": "8825f6ab-607d-4de1-a790-b32b2b187f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "[\n",
            "\t{\"bbox_2d\": [74, 361, 324, 809], \"label\": \"plate/dish\"},\n",
            "\t{\"bbox_2d\": [301, 134, 480, 395], \"label\": \"plate/dish\"},\n",
            "\t{\"bbox_2d\": [486, 188, 743, 571], \"label\": \"plate/dish\"},\n",
            "\t{\"bbox_2d\": [320, 663, 566, 956], \"label\": \"plate/dish\"},\n",
            "\t{\"bbox_2d\": [554, 575, 808, 901], \"label\": \"plate/dish\"},\n",
            "\t{\"bbox_2d\": [741, 440, 999, 900], \"label\": \"plate/dish\"},\n",
            "\t{\"bbox_2d\": [692, 181, 992, 495], \"label\": \"plate/dish\"},\n",
            "\t{\"bbox_2d\": [0, 103, 200, 487], \"label\": \"plate/dish\"},\n",
            "\t{\"bbox_2d\": [735, 332, 833, 443], \"label\": \"scallop\"},\n",
            "\t{\"bbox_2d\": [815, 310, 917, 442], \"label\": \"scallop\"},\n",
            "\t{\"bbox_2d\": [835, 201, 918, 309], \"label\": \"scallop\"},\n",
            "\t{\"bbox_2d\": [753, 207, 856, 322], \"label\": \"scallop\"},\n",
            "\t{\"bbox_2d\": [692, 236, 775, 329], \"label\": \"scallop\"},\n",
            "\t{\"bbox_2d\": [885, 300, 980, 426], \"label\": \"scallop\"},\n",
            "\t{\"bbox_2d\": [161, 0, 304, 353], \"label\": \"wine bottle\"},\n",
            "\t{\"bbox_2d\": [615, 73, 731, 210], \"label\": \"bowl\"},\n",
            "\t{\"bbox_2d\": [255, 81, 333, 221], \"label\": \"bowl\"},\n",
            "\t{\"bbox_2d\": [0, 800, 102, 999], \"label\": \"bowl\"},\n",
            "\t{\"bbox_2d\": [618, 938, 764, 999], \"label\": \"bowl\"},\n",
            "\t{\"bbox_2d\": [982, 444, 999, 563], \"label\": \"bowl\"},\n",
            "\t{\"bbox_2d\": [637, 50, 714, 182], \"label\": \"spoon\"},\n",
            "\t{\"bbox_2d\": [729, 0, 865, 194], \"label\": \"coconut drink\"},\n",
            "\t{\"bbox_2d\": [96, 812, 220, 999], \"label\": \"cup\"},\n",
            "\t{\"bbox_2d\": [728, 842, 846, 999], \"label\": \"cup\"},\n",
            "\t{\"bbox_2d\": [800, 801, 836, 999], \"label\": \"chopsticks\"},\n",
            "\t{\"bbox_2d\": [902, 434, 999, 466], \"label\": \"chopsticks\"}\n",
            "]\n",
            "```\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MissingSchema",
          "evalue": "Invalid URL './dining_table.png': No scheme supplied. Perhaps you meant https://./dining_table.png?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2544552915.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         p.prepare(\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             raise MissingSchema(\n\u001b[0m\u001b[1;32m    439\u001b[0m                 \u001b[0;34mf\"Invalid URL {url!r}: No scheme supplied. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;34mf\"Perhaps you meant https://{url}?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL './dining_table.png': No scheme supplied. Perhaps you meant https://./dining_table.png?"
          ]
        }
      ],
      "source": [
        "# Example 1: Detecting different objects on a dining table\n",
        "\n",
        "# You can specify the categories of the instances you want to locate (negative categories are also supported and will be skipped during generation)\n",
        "prompt = 'locate every instance that belongs to the following categories: \"plate/dish, scallop, wine bottle, tv, bowl, spoon, air conditioner, coconut drink, cup, chopsticks, person\". Report bbox coordinates in JSON format.'\n",
        "img_url = \"./dining_table.png\"\n",
        "model_response = inference_with_openai_api(img_url, prompt)\n",
        "print(model_response)\n",
        "\n",
        "#response = requests.get(img_url)\n",
        "#response.raise_for_status()\n",
        "#image = Image.open(BytesIO(response.content))\n",
        "\n",
        "# 读取本地文件\n",
        "with open(img_url, \"rb\") as f:\n",
        "    image = Image.open(f)\n",
        "    image.load()  # 确保完全读入\n",
        "\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_bounding_boxes(image, model_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea4a73f",
      "metadata": {
        "id": "cea4a73f"
      },
      "outputs": [],
      "source": [
        "# Example 2: Detecting different objects in crowded scenes\n",
        "\n",
        "# You can specify the categories of the instances you want to locate (negative categories are also supported and will be skipped during generation)\n",
        "prompt = 'Locate every instance that belongs to the following categories: \"head, hand, man, woman, glasses\". Report bbox coordinates in JSON format.'\n",
        "img_url = \"./assets/spatial_understanding/lots_of_people.jpeg\"\n",
        "model_response = inference_with_openai_api(img_url, prompt)\n",
        "print(model_response)\n",
        "\n",
        "response = requests.get(img_url)\n",
        "response.raise_for_status()\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_bounding_boxes(image, model_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6acb438",
      "metadata": {
        "id": "c6acb438"
      },
      "outputs": [],
      "source": [
        "# Example 3: Detecting different objects in a 4K drone-view image\n",
        "\n",
        "# You can specify the categories of the instances you want to locate (negative categories are also supported and will be skipped during generation)\n",
        "prompt = 'Locate every instance that belongs to the following categories: \"car, bus, bicycle, pedestrian\". Report bbox coordinates in JSON format.'\n",
        "img_url = \"./assets/spatial_understanding/lots_of_cars.png\"\n",
        "model_response = inference_with_openai_api(img_url, prompt)\n",
        "print(model_response)\n",
        "\n",
        "response = requests.get(img_url)\n",
        "response.raise_for_status()\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_bounding_boxes(image, model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7363e9e7",
      "metadata": {
        "id": "7363e9e7"
      },
      "source": [
        "In addition, you can specify more complex requirements in grounding tasks by defining the output format, such as including additional key information like object attributes, descriptions, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f44a82c",
      "metadata": {
        "id": "2f44a82c"
      },
      "outputs": [],
      "source": [
        "# Example 4: Detecting vehicles with additional key information\n",
        "\n",
        "# You can set the output format to include additional key information like object attributes, descriptions, etc.\n",
        "prompt = 'locate every instance that belongs to the following categories: \"vehicle\". For each vehicle, report bbox coordinates, vehicle type and vehicle color in JSON format like this: {\"bbox_2d\": [x1, y1, x2, y2], \"label\": \"vehicle\", \"type\": \"car, bus, truck, bicycle, ...\", \"color\": \"vehicle_color\"}'\n",
        "img_url = \"./assets/spatial_understanding/drone_cars2.png\"\n",
        "model_response = inference_with_openai_api(img_url, prompt)\n",
        "print(model_response)\n",
        "\n",
        "response = requests.get(img_url)\n",
        "response.raise_for_status()\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_bounding_boxes(image, model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda1639e",
      "metadata": {
        "id": "fda1639e"
      },
      "source": [
        "Similarly, point-based grounding also supports the above feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acae6082",
      "metadata": {
        "id": "acae6082"
      },
      "outputs": [],
      "source": [
        "# Example 5: Pointing out the people inside a football field and output their role and shirt color.\n",
        "\n",
        "# You can also set the output format to include additional key information like object attributes, descriptions, etc in point-based grounding.\n",
        "prompt = '''Locate every person inside the football field with points, report their point coordinates, role(player, referee or unknown) and shirt color in JSON format like this: {\"point_2d\": [x, y], \"label\": \"person\", \"role\": \"player/referee/unknown\", \"shirt_color\": \"the person's shirt color\"}'''\n",
        "img_url = \"./assets/spatial_understanding/football_field.jpg\"\n",
        "model_response = inference_with_openai_api(img_url, prompt)\n",
        "print(model_response)\n",
        "\n",
        "response = requests.get(img_url)\n",
        "response.raise_for_status()\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_points_json(image, model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c340c27",
      "metadata": {
        "id": "9c340c27"
      },
      "source": [
        "#### 2. Detect certain object in the image\n",
        "\n",
        "Furthermore, based on this capability, we can prompt the model with specific questions that require spatial reasoning, such as \"Is object A above or below object B?\" or \"Please describe the object closest to object C.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e969b5-de9f-4efc-b8ae-a95ca441d639",
      "metadata": {
        "ExecutionIndicator": {
          "show": false
        },
        "execution": {
          "iopub.execute_input": "2025-01-29T12:19:03.259891Z",
          "iopub.status.busy": "2025-01-29T12:19:03.259307Z",
          "iopub.status.idle": "2025-01-29T12:19:17.272268Z",
          "shell.execute_reply": "2025-01-29T12:19:17.271760Z",
          "shell.execute_reply.started": "2025-01-29T12:19:03.259862Z"
        },
        "tags": [],
        "id": "94e969b5-de9f-4efc-b8ae-a95ca441d639"
      },
      "outputs": [],
      "source": [
        "image_path = \"./assets/spatial_understanding/spatio_case1.jpg\"\n",
        "prompt = \"Which object, in relation to your current position, holds the farthest placement in the image?\\nAnswer options:\\nA.chair\\nB.plant\\nC.window\\nD.tv stand.\"\n",
        "response = inference_with_api(image_path, prompt)\n",
        "\n",
        "# prompt in chinese\n",
        "prompt = \"框出每一个小蛋糕的位置，以json格式输出所有的坐标\"\n",
        "# prompt in english\n",
        "prompt = \"Outline the position of each small cake and output all the coordinates in JSON format.\"\n",
        "\n",
        "## Use openai-style api to inference.\n",
        "model_response = inference_with_openai_api(image_path, prompt)\n",
        "print(model_response)\n",
        "\n",
        "image = Image.open(image_path)\n",
        "print(image.size)\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_bounding_boxes(image, model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83111604",
      "metadata": {
        "id": "83111604"
      },
      "source": [
        "Further, you can search for a specific object by using a short phrase or sentence to describe it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f34464d-d7c7-4dbe-81d1-b811eceb9c5e",
      "metadata": {
        "ExecutionIndicator": {
          "show": false
        },
        "execution": {
          "iopub.execute_input": "2025-01-29T12:20:58.885718Z",
          "iopub.status.busy": "2025-01-29T12:20:58.885124Z",
          "iopub.status.idle": "2025-01-29T12:21:00.739805Z",
          "shell.execute_reply": "2025-01-29T12:21:00.739195Z",
          "shell.execute_reply.started": "2025-01-29T12:20:58.885697Z"
        },
        "tags": [],
        "id": "7f34464d-d7c7-4dbe-81d1-b811eceb9c5e"
      },
      "outputs": [],
      "source": [
        "image_path = \"./assets/spatial_understanding/cakes.png\"\n",
        "\n",
        "# prompt in chinses\n",
        "prompt = \"定位最右上角的棕色蛋糕，以JSON格式输出其bbox坐标\"\n",
        "# prompt in english\n",
        "prompt = \"Locate the top right brown cake, output its bbox coordinates using JSON format.\"\n",
        "\n",
        "## Use openai-style api to inference.\n",
        "model_response = inference_with_openai_api(image_path, prompt)\n",
        "print(model_response)\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_bounding_boxes(image, model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08ae7892",
      "metadata": {
        "id": "08ae7892"
      },
      "source": [
        "For spatial pointing tasks, Qwen3-VL now support these formats:\n",
        "\n",
        "In addition to the above mentioned bbox format [x1, y1, x2, y2], Qwen2.5-VL also supports point-based grounding. You can point to a specific object and the model is trained to output xml-style results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c900f9dd",
      "metadata": {
        "ExecutionIndicator": {
          "show": false
        },
        "execution": {
          "iopub.execute_input": "2025-01-29T12:22:08.238982Z",
          "iopub.status.busy": "2025-01-29T12:22:08.238698Z",
          "iopub.status.idle": "2025-01-29T12:22:12.129376Z",
          "shell.execute_reply": "2025-01-29T12:22:12.128799Z",
          "shell.execute_reply.started": "2025-01-29T12:22:08.238962Z"
        },
        "tags": [],
        "id": "c900f9dd"
      },
      "outputs": [],
      "source": [
        "image_path = \"./assets/spatial_understanding/cakes.png\"\n",
        "\n",
        "# prompt in chinese\n",
        "prompt = \"以点的形式定位图中桌子远处的擀面杖，以XML格式输出其坐标\"\n",
        "# prompt in english\n",
        "prompt = \"point to the rolling pin on the far side of the table, output its coordinates in XML format <points x y>object</points>\"\n",
        "\n",
        "## Use openai-style api to inference.\n",
        "model_response = inference_with_openai_api(image_path, prompt)\n",
        "print(model_response)\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_points(image, model_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5352ad",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2025-01-29T12:24:36.427415Z",
          "iopub.status.busy": "2025-01-29T12:24:36.427110Z",
          "iopub.status.idle": "2025-01-29T12:24:38.098639Z",
          "shell.execute_reply": "2025-01-29T12:24:38.097988Z",
          "shell.execute_reply.started": "2025-01-29T12:24:36.427395Z"
        },
        "tags": [],
        "id": "7f5352ad"
      },
      "outputs": [],
      "source": [
        "image_path = \"./assets/spatial_understanding/spatio_case2_aff.png\"\n",
        "prompt = \"Locate the free space on the white table on the right in this image. Output the point coordinates in JSON format.\"\n",
        "response = inference_with_api(image_path, prompt)\n",
        "\n",
        "# prompt in chinese\n",
        "prompt = \"框出图中纸狐狸的影子，以json格式输出其bbox坐标\"\n",
        "# prompt in english\n",
        "prompt = \"Locate the shadow of the paper fox, report the bbox coordinates in JSON format.\"\n",
        "\n",
        "## Use openai-style api to inference.\n",
        "model_response = inference_with_openai_api(image_path, prompt)\n",
        "print(model_response)\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_bounding_boxes(image, model_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9e934f",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2025-01-29T12:24:36.427415Z",
          "iopub.status.busy": "2025-01-29T12:24:36.427110Z",
          "iopub.status.idle": "2025-01-29T12:24:38.098639Z",
          "shell.execute_reply": "2025-01-29T12:24:38.097988Z",
          "shell.execute_reply.started": "2025-01-29T12:24:36.427395Z"
        },
        "tags": [],
        "id": "ad9e934f"
      },
      "outputs": [],
      "source": [
        "image_path = \"./assets/spatial_understanding/spatio_case2_aff2.png\"\n",
        "prompt = \"Can the speaker fit behind the guitar?\"\n",
        "response = inference_with_api(image_path, prompt)\n",
        "\n",
        "# prompt in chinese\n",
        "prompt = \"框出图中见义勇为的人，以json格式输出其bbox坐标\"\n",
        "# prompt in english\n",
        "prompt = \"Locate the person who act bravely, report the bbox coordinates in JSON format.\"\n",
        "\n",
        "## Use openai-style api to inference.\n",
        "model_response = inference_with_openai_api(image_path, prompt)\n",
        "print(model_response)\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_bounding_boxes(image, model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "851bf964",
      "metadata": {
        "id": "851bf964"
      },
      "source": [
        "#### 3. Integrate Spatial Reasoning and Action Planning\n",
        "\n",
        "This advanced task integrates the understanding of spatial relationships and affordances. The model must synthesize these capabilities to select the correct action that achieves a goal, effectively reasoning like an embodied agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdd74c94",
      "metadata": {
        "ExecutionIndicator": {
          "show": false
        },
        "tags": [],
        "id": "fdd74c94"
      },
      "outputs": [],
      "source": [
        "image_path = \"./assets/spatial_understanding/spatio_case2_plan.png\"\n",
        "prompt = \"What color arrow should the robot follow to move the apple in between the green can and the orange? Choices: A. Red. B. Blue. C. Green. D. Orange.\"\n",
        "response = inference_with_api(image_path, prompt)\n",
        "\n",
        "# prompt in chinese\n",
        "prompt = \"如果太阳很刺眼，我应该用这张图中的什么物品，框出该物品在图中的bbox坐标，并以json格式输出\"\n",
        "# prompt in english\n",
        "prompt = \"If the sun is very glaring, which item in this image should I use? Please locate it in the image with its bbox coordinates and its label and output in JSON format.\"\n",
        "\n",
        "## Use openai-style api to inference.\n",
        "model_response = inference_with_openai_api(url, prompt)\n",
        "print(model_response)\n",
        "\n",
        "image = Image.open(url)\n",
        "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "plot_bounding_boxes(image, model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d53246e",
      "metadata": {
        "id": "8d53246e"
      },
      "source": [
        "#### 7. Known Limitations\n",
        "* When the number of instances in a particular category significantly exceeds 40-50 or when objects are densely packed, the model may enter an endless generation loop.\n",
        "* The model occasionally detects a cluster of closely spaced objects as a single bounding box. This issue can be mitigated by refining the input prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fca8fc6",
      "metadata": {
        "id": "0fca8fc6"
      },
      "source": [
        "#### 8. spatial understanding with designed system prompt\n",
        "The above usage is based on the default system prompt. You can also change the system prompt to obtain other output format like plain text.\n",
        "Qwen3-VL now support these formats:\n",
        "* bbox-format: JSON (default)\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import decord\n",
        "from decord import VideoReader, cpu\n",
        "\n",
        "\n",
        "def download_video(url, dest_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(dest_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8096):\n",
        "            f.write(chunk)\n",
        "    print(f\"Video downloaded to {dest_path}\")\n",
        "\n",
        "\n",
        "def get_video_frames(video_path, num_frames=128, cache_dir='./assets/spatial_understanding/'):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "* point-format: JSON (default)\n",
        "\n",
        "    frames_cache_file = os.path.join(cache_dir, f'{video_hash}_{num_frames}_frames.npy')\n",
        "    timestamps_cache_file = os.path.join(cache_dir, f'{video_hash}_{num_frames}_timestamps.npy')\n",
        "\n",
        "    if os.path.exists(frames_cache_file) and os.path.exists(timestamps_cache_file):\n",
        "        frames = np.load(frames_cache_file)\n",
        "        timestamps = np.load(timestamps_cache_file)\n",
        "        return video_file_path, frames, timestamps\n",
        "\n",
        "    vr = VideoReader(video_file_path, ctx=cpu(0))\n",
        "    total_frames = len(vr)\n",
        "\n",
        "    indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
        "    frames = vr.get_batch(indices).asnumpy()\n",
        "    timestamps = np.array([vr.get_frame_timestamp(idx) for idx in indices])\n",
        "\n",
        "    np.save(frames_cache_file, frames)\n",
        "    np.save(timestamps_cache_file, timestamps)\n",
        "    \n",
        "    return video_file_path, frames, timestamps\n",
        "\n",
        "\n",
        "def create_image_grid(images, num_columns=8):\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    num_rows = math.ceil(len(images) / num_columns)\n",
        "\n",
        "    img_width, img_height = pil_images[0].size\n",
        "    grid_width = num_columns * img_width\n",
        "    grid_height = num_rows * img_height\n",
        "    grid_image = Image.new('RGB', (grid_width, grid_height))\n",
        "\n",
        "    for idx, image in enumerate(pil_images):\n",
        "        row_idx = idx // num_columns\n",
        "        col_idx = idx % num_columns\n",
        "        position = (col_idx * img_width, row_idx * img_height)\n",
        "        grid_image.paste(image, position)\n",
        "\n",
        "    return grid_image\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "qwen_mm_data",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}